1. Для чего и в каких случаях полезны различные варианты усреднения для метрик качества классификации: micro, macro, weighted?
Самый простой и стандардтный вариант - это macro. В нем берутся усредненные значения для каждого из классов. 
Macro не делает различий между более и менее важными классами. Также, macro фокусируется на распозновании класса, а не усредненных значениях precision/recall для всей модели. 
Micro вариант усредняет precision/recall для всей многоклассовой модели, но не уделяет внимания распределению классов в данных. Weighted вариант присваивает веса для каждого из классов, исходя из их распределения в данных.  

2. В чём разница между моделями xgboost, lightgbm и catboost или какие их основные особенности?
XGBoost - это eXtreme Gradient Boosting, т.е. в основе данной модели лежит градиентный бустинг. XGBoost посчитывает похожеть между элементами и производит стрижку результирующих деревьев, основываясь на гиперпараметр гамма. 
Прирост информации в XGBoost подсчитывается, как разница между суммой веток и вершины.
LightGBM - это облегченная версия градиентного бустинга, которая заостряет внимание на ошибках и не использует всю выборку.
Также, эта модель группирует разреженные признаки, типичные для бинарного формата машинного обучения, а также признаки с близкими диапазонами значений.
Catboost - это быстрая модель градиентного бустинга, построенная на симметричных разветвлениях. Выдерживая симметрию, модель решает проблемы классификации быстрее аналогов. 
В дополнение, эта модель сама переводит признаки в нужный формат без использования функции get_dummies и имеет встроенные алгоритмы, препятствующие переобучению.  
